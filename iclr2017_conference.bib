@article{maml,
  title={Model-agnostic metalearning for fast adaptation of deep networks},
  author={Finn, C. and Abbeel, P. and Levine, S.},
  journal={ICML},
  year={2017}
}

@article{rl2,
  title={RL2: Fast reinforcement learning via slow reinforcement learning},
  author={Duan, Y. and Schulman, J. and Chen, X. and Bartlett, P. L. and Sutskever, I. and Abbeel, P.},
  journal={arXiv preprint arXiv:1611.02779},
  year={2016}
}

@article{reinforce,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J.},
  journal={Machine Learning 8, 3-4 (1992), 229–256},
  year={1992}
}

@article{ddpg,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, T. and Hunt, J. and Pritzel, A. and Heess, N. and Erez, T. and Tassa, Y. and Silver, D. and Wierstra, D. },
  journal={arXiv:1509.02971},
  year={2015}
}

@article{ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, J. and Wolski, F. and Dhariwal, P. and Radford, A. and Klimov, O.},
  journal={arXiv:1707.06347},
  year={2017}
}


@inproceedings{kingma2014adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P. and Ba, Jimmy},
  booktitle={Proceedings of the 3rd International Conference on Learning Representations (ICLR)},
  year={2014}
}

@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={39},
  pages={1--40},
  year={2016}
}

@article{schaal1999imitation,
  title={Is imitation learning the route to humanoid robots?},
  author={Schaal, Stefan},
  journal={Trends in cognitive sciences},
  volume={3},
  number={6},
  pages={233--242},
  year={1999},
  publisher={Elsevier}
}

@article{argall2009survey,
  title={A survey of robot learning from demonstration},
  author={Argall, Brenna D and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
  journal={Robotics and autonomous systems},
  volume={57},
  number={5},
  pages={469--483},
  year={2009},
  publisher={Elsevier}
}

@book{calinon2009robot,
  title={Robot programming by demonstration},
  author={Calinon, Sylvain},
  year={2009},
  publisher={EPFL Press}
}


@article{a3c,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  journal={arXiv:1602.01783},
  year={2016}
}

@article{boltzman,
  title={Exploration Strategies for Model-based Learning in Multi-agent Systems},
  author={Carmel, D. and Markovitch, S.},
  journal={Autonomous Agents and Multi-Agent Systems},
  year={1999}
}


@article{hash,
  title={#exploration: A study of count-based exploration for deep reinforcement learning.},
  author={Tang, H. and Houthooft, R. and Foote, D. and Stooke, A. and Chen, X. and Duan, Y. and Schulman, J. and De Turck, F. and Abbeel, P.},
  journal={arXiv:1611.04717},
  year={2016}
}

@article{vime,
  title={Vime: Variational information maximizing exploration},
  author={Houthooft, R. and Chen, X. and Duan, Y. and Schulman, J. and De Turck, F. and Abbeel, P.},
  journal={NIPS},
  year={2016}
}

@article{bootstrapdqn,
  title={Deep exploration via bootstrapped dqn},
  author={Osband, I. and Blundell, C. and Pritzel, A. and Van Roy, B.},
  journal={NIPS},
  year={2016}
}

@article{stadie,
  title={Incentivizing exploration in reinforcement learning with deep predictive models},
  author={Stadie, Bradly C. and Levine, S. and Abbeel, P.},
  journal={arXiv:1507.00814},
  year={2015}
}

@article{countbased,
  title={Unifying count based exploration and intrinsic motivation},
  author={Bellemare, M. and Srinivasan, S. and Ostrovski, G. and Schaul, T. and Saxton, D. and Munos, R.},
  journal={NIPS},
  year={2016}
}

@article{countbased2,
  title={Count-based exploration with neural density models},
  author={Ostrovski, G. and Bellemare, M. G. and Oord, A. v. d. and Munos, R.},
  journal={arXiv:1703.01310},
  year={2017}
}

@article{beb,
  title={Near-bayesian exploration in polynomial time},
  author={Kolter, J. Z. and Ng, A. Y.},
  journal={ICML},
  year={2009}
}

@article{learntorl,
  title={Learning to reinforcement learn},
  author={Wang, J. X. and Kurth-Nelson, Z. and Tirumala, D. and Soyer, H. and Leibo, J. Z. and Munos, R. and Blundell, C. and Kumaran, D. and Botvinick, M.},
  journal={arXiv:1611.05763},
  year={2016}
}

@article{kearns,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, M. J. and Singh, S. P.},
  journal={Machine Learning},
  year={2002}
}

@article{rmax,
  title={R-max - a general polynomial time algorithm for near-optimal reinforcement learning},
  author={Brafman, R. I. and Tennenholtz, M.},
  journal={Journal of Machine Learning Research},
  year={2002}
}

@article{hrl,
  title={Stochastic neural networks for hierarchical reinforcement learning},
  author={Florensa, C. and Duan, Y. and Abbeel, P.},
  journal={Int. Conf. on Learning Representations},
  year={2017}
}

@article{feudal,
  title={Feudal networks for hierarchical reinforcement learning},
  author={Vezhnevets, A. S. and Osindero, S. and Schaul, T. and Heess, N. and Jaderberg, M. and Silver, D. and Kavukcuoglu, K.},
  journal={arXiv preprint arXiv:1703.01161},
  year={2017}
}

@article{optioncritic,
  title={The option-critic architecture},
  author={Bacon, P. and Precup, D.},
  journal={In NIPS Deep RL Workshop},
  year={2015}
}

@article{minecraft,
  title={A Deep Hierarchical Approach to Lifelong Learning in Minecraft},
  author={Tessler, C. Givony, S. and Zahavy, T. and Mankowitz, D.J. and Mannor, S.},
  journal={arXiv:1604.07255},
  year={2016}
}

@article{progressive,
  title={Progressive neural networks},
  author={Rusu, A. A.  and Rabinowitz, N. C. and Desjardins, G. and Soyer, H. and Kirkpatrick, J. and Kavukcuoglu, K. and Pascanu, R. and Hadsell, R.},
  journal={CoRR, vol. abs/1606.04671},
  year={2016}
}

@article{bromley1993signature,
  title={Signature verification using a “Siamese” time delay neural network},
  author={Bromley, Jane and Bentz, James W and Bottou, L{\'e}on and Guyon, Isabelle and LeCun, Yann and Moore, Cliff and S{\"a}ckinger, Eduard and Shah, Roopak},
  journal={International Journal of Pattern Recognition and Artificial Intelligence},
  volume={7},
  number={04},
  pages={669--688},
  year={1993},
  publisher={World Scientific}
}

@inproceedings{chopra2005learning,
  title={Learning a similarity metric discriminatively, with application to face verification},
  author={Chopra, Sumit and Hadsell, Raia and LeCun, Yann},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  volume={1},
  pages={539--546},
  year={2005},
  organization={IEEE}
}

@inproceedings{yang2007cross,
  title={Cross-domain video concept detection using adaptive svms},
  author={Yang, Jun and Yan, Rong and Hauptmann, Alexander G},
  booktitle={Proceedings of the 15th ACM international conference on Multimedia},
  pages={188--197},
  year={2007},
  organization={ACM}
}

@inproceedings{aytar2011tabula,
  title={Tabula rasa: Model transfer for object category detection},
  author={Aytar, Yusuf and Zisserman, Andrew},
  booktitle={2011 International Conference on Computer Vision},
  pages={2252--2259},
  year={2011},
  organization={IEEE}
}

@article{duan2012learning,
  title={Learning with augmented features for heterogeneous domain adaptation},
  author={Duan, Lixin and Xu, Dong and Tsang, Ivor},
  journal={arXiv preprint arXiv:1206.4660},
  year={2012}
}

@article{hoffman2013efficient,
  title={Efficient learning of domain-invariant image representations},
  author={Hoffman, Judy and Rodner, Erik and Donahue, Jeff and Darrell, Trevor and Saenko, Kate},
  journal={arXiv preprint arXiv:1301.3224},
  year={2013}
}


@article{long2015learning,
  title={Learning transferable features with deep adaptation networks},
  author={Long, Mingsheng and Wang, Jianmin},
  journal={CoRR, abs/1502.02791},
  volume={1},
  pages={2},
  year={2015}
}

@article{mansour2009domain,
  title={Domain adaptation: Learning bounds and algorithms},
  author={Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
  journal={arXiv preprint arXiv:0902.3430},
  year={2009}
}



@article{ganin2014unsupervised,
  title={Unsupervised domain adaptation by backpropagation},
  author={Ganin, Yaroslav and Lempitsky, Victor},
  journal={arXiv preprint arXiv:1409.7495},
  year={2014}
}

@inproceedings{kulis2011you,
  title={What you saw is not what you get: Domain adaptation using asymmetric kernel transforms},
  author={Kulis, Brian and Saenko, Kate and Darrell, Trevor},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on},
  pages={1785--1792},
  year={2011},
  organization={IEEE}
}



@inproceedings{donahue2014decaf,
  title={DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition.},
  author={Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
  booktitle={ICML},
  pages={647--655},
  year={2014}
}



@article{tzeng2014deep,
  title={Deep domain confusion: Maximizing for domain invariance},
  author={Tzeng, Eric and Hoffman, Judy and Zhang, Ning and Saenko, Kate and Darrell, Trevor},
  journal={arXiv preprint arXiv:1412.3474},
  year={2014}
}


@article{tzeng2015towards,
  title={Towards Adapting Deep Visuomotor Representations from Simulated to Real Environments},
  author={Tzeng, Eric and Devin, Coline and Hoffman, Judy and Finn, Chelsea and Peng, Xingchao and Abbeel, Pieter and Levine, Sergey and Saenko, Kate and Darrell, Trevor},
  journal={arXiv preprint arXiv:1511.07111},
  year={2015}
}

@article{abbeel2010autonomous,
  title={Autonomous helicopter aerobatics through apprenticeship learning},
  author={Abbeel, Pieter and Coates, Adam and Ng, Andrew Y},
  journal={The International Journal of Robotics Research},
  year={2010},
  publisher={SAGE Publications}
}

@article{dqn,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{schulman2015high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}


@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2672--2680},
  year={2014}
}


@inproceedings{nr-airl-00,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, A. and Russell, S. and others},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2000}
}


@inproceedings{zmbd-meirl-08,
   author = {B. Ziebart and A. Maas
            and J. A. Bagnell and A. K. Dey},
   title = {Maximum Entropy Inverse Reinforcement Learning},
   year = {2008},
   booktitle = {AAAI Conference on Artificial Intelligence},
}

@inproceedings{rbbc-bsp-07,
  title={Boosting structured prediction for imitation learning},
  author={Ratliff, N. and Bradley, D. and Bagnell, J. A. and Chestnutt, J.},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2007},
}



@inproceedings{lpk-gpirl-11,
  title={Nonlinear inverse reinforcement learning with gaussian processes},
  author={Levine, S. and Popovic, Z. and Koltun, V.},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2011}
}

@inproceedings{an-alirl-04,
 author = {Abbeel, P. and Ng, A.},
 title = {Apprenticeship learning via inverse reinforcement learning},
 booktitle = {International Conference on Machine Learning (ICML)},
 year = {2004},
 }

@inproceedings{ra-birl-07,
  title={Bayesian inverse reinforcement learning},
  author={Ramachandran, D. and Amir, E.},
   booktitle = {AAAI Conference on Artificial Intelligence},
  volume={51},
  year={2007}
}

@INPROCEEDINGS{drbts-dlmioc-15,
    AUTHOR    = {A. Doerr AND N. Ratliff AND J. Bohg AND M. Toussaint AND S. Schaal},
    TITLE     = {Direct Loss Minimization Inverse Optimal Control},
    BOOKTITLE = {Proceedings of Robotics: Science and Systems (R:SS)},
    YEAR      = {2015},
    ADDRESS   = {Rome, Italy},
    MONTH     = {July}
}



@article{wop-dirl-15,
  author    = {M. Wulfmeier and
               P. Ondruska and
               I. Posner},
  title     = {Maximum Entropy Deep Inverse Reinforcement Learning},
  journal={arXiv preprint arXiv:1507.04888},
  year      = {2015},
  timestamp = {Sun, 02 Aug 2015 18:42:02 +0200},
}

@inproceedings{rbz-mmp-06,
 author = {Ratliff, N. and Bagnell, J. A. and Zinkevich, M. A.},
 title = {Maximum margin planning},
  booktitle = {International Conference on Machine Learning (ICML)},
 year = {2006},
 }


@inproceedings{bkp-reirl-11,
  author = "Boularias, A. and  Kober, J. and  Peters, J.",
  year = "2011",
  title = "Relative Entropy Inverse Reinforcement Learning",
  booktitle = "International Conference on Artificial Intelligence and Statistics (AISTATS)"
}

@inproceedings{kprs-lofm-13,
  author = "Kalakrishnan, M. and  Pastor, P. and  Righetti, L. and  Schaal, S.",
  year = "2013",
  title = "Learning Objective Functions for Manipulation",
  booktitle = "International Conference on Robotics and Automation (ICRA)"
}


@article{nehaniv2007nine,
  title={Nine billion correspondence problems},
  author={Nehaniv, Chrystopher L},
  journal={Imitation and Social Learning in Robots, Humans and Animals: Behavioural, Social and Communicative Dimensions, Cambridge University Press},
  volume={8},
  pages={10},
  year={2007}
}

@article{gergely2002developmental,
  title={Developmental psychology: Rational imitation in preverbal infants},
  author={Gergely, Gy{\"o}rgy and Bekkering, Harold and Kir{\'a}ly, Ildik{\'o}},
  journal={Nature},
  volume={415},
  number={6873},
  pages={755--755},
  year={2002},
  publisher={Nature Publishing Group}
}

@article{carpenter2002understanding,
  title={Understanding “prior intentions” enables two--year--olds to imitatively learn a complex task},
  author={Carpenter, Malinda and Call, Josep and Tomasello, Michael},
  journal={Child development},
  volume={73},
  number={5},
  pages={1431--1441},
  year={2002},
  publisher={Wiley Online Library}
}

@article{gioioso2013object,
  title={An object-based approach to map human hand synergies onto robotic hands with dissimilar kinematics},
  author={Gioioso, G and Salvietti, G and Malvezzi, M and Prattichizzo, D},
  journal={Robotics: Science and Systems VIII},
  pages={97},
  year={2013},
  publisher={MIT Press}
}

@inproceedings{shon2005learning,
  title={Learning shared latent structure for image synthesis and robotic imitation},
  author={Shon, Aaron and Grochow, Keith and Hertzmann, Aaron and Rao, Rajesh P},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1233--1240},
  year={2005}
}

@article{gupta2016learning,
  title={Learning Dexterous Manipulation for a Soft Robotic Hand from Human Demonstration},
  author={Gupta, Abhishek and Eppner, Clemens and Levine, Sergey and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1603.06348},
  year={2016}
}


@article{calinon2007learning,
  title={On learning, representing, and generalizing a task in a humanoid robot},
  author={Calinon, Sylvain and Guenter, Florent and Billard, Aude},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume={37},
  number={2},
  pages={286--298},
  year={2007},
  publisher={IEEE}
}

@article{nehaniv2001like,
  title={Like me?-measures of correspondence and imitation},
  author={Nehaniv, Chrystopher L and Dautenhahn, Kerstin},
  journal={Cybernetics \& Systems},
  volume={32},
  number={1-2},
  pages={11--51},
  year={2001},
  publisher={Taylor \& Francis}
}

@inproceedings{pomerleau1989alvinn,
  title={ALVINN: An Autonomous Land Vehicle in a Neural Network},
  author={Pomerleau, Dean A},
  booktitle={Advances in Neural Information Processing Systems},
  pages={305--313},
  year={1989}
}

@article{BojarskiTDFFGJM16,
  author    = {Mariusz Bojarski and
               Davide Del Testa and
               Daniel Dworakowski and
               Bernhard Firner and
               Beat Flepp and
               Prasoon Goyal and
               Lawrence D. Jackel and
               Mathew Monfort and
               Urs Muller and
               Jiakai Zhang and
               Xin Zhang and
               Jake Zhao and
               Karol Zieba},
  title     = {End to End Learning for Self-Driving Cars},
  journal   = {CoRR},
  volume    = {abs/1604.07316},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.07316},
  timestamp = {Mon, 02 May 2016 18:22:52 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BojarskiTDFFGJM16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}


@inproceedings{ross2011reduction,
  title={A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning.},
  author={Ross, St{\'e}phane and Gordon, Geoffrey J and Bagnell, Drew},
  booktitle={AISTATS},
  pages={6},
  year={2011}
}


@article{sch1,
author = {Koutník, Jan and Cuccu, Giuseppe  and Schmidhuber, Jürgen and Gomez, Faustino},
journal = {GECCO 2013 },
pages = {1061--1068 },
title = {Evolving large-scale neural networks for vision-based reinforcement learning},
volume = {15},
year = {2013}
}

@article{ho,
author = {Ho, J. and Ermon, S.},
journal = {arXiv pre-print: 1606.03476},
pages = {1061--1068 },
title = {Generative adversarial imitation learning},
year = {2016}
}


@article{ziebart,
author = {Ziebart, B.D. and Maas, A.L. and Bagnell, J.A. and Dey, A.K.},
journal = {AAAI Conference on Artificial Intelligence},
title = {Maximum entropy inverse reinforcement learning},
year = {2008}
}

@article{finn,
author = {Finn, C. and Levine, S. and Abbeel, P.},
journal = {ICML},
title = {Guided cost learning: Deep inverse optimal control via policy
optimization},
year = {2016}
}


@article{trpo,
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
journal = {Arxiv preprint 1502.05477},
title = {Trust region policy optimization},
year = {2015}
}

@article{dann,
author = {Ganin, Y. and Lempitsky, V.},
journal = {Arxiv preprint 1409.7495},
title = {Unsupervised Domain Adaptation
by Backpropagation},
year = {2014}
}

@article{mutinf0,
author = {J. S. Bridle and A. J. Heading and D. J. MacKay},
journal = {NIPS},
title = {Unsupervised classifiers, mutual information and ’phantom targets’},
year = {1992}
}


@article{mutinf1,
author = {D. Barber and F. V. Agakov},
journal = {NIPS},
title = {Kernelized infomax clustering},
year = {2005}
}

@article{mutinf2,
author = {A. Krause and P. Perona, and R. G. Gomes},
journal = {NIPS},
title = {Discriminative clustering by regularized information maximization},
year = {2010}
}

@article{infogan,
author = {Xi Chen and Yan Duan and Rein Houthooft and John Schulman and Ilya Sutskever and Pieter Abbeel},
journal = {NIPS},
title = {Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
year = {2016}
}

% ===================================== Ge Added Bib Entries here =====================================

@article{Ngo2012,
abstract = {Artificial curiosity tries to maximize learning progress.We apply this concept to a physical system. Our Katana robot arm curiously plays with wooden blocks, using vision, reaching, and grasping. It is intrinsically motivated to explore its world. As a by-product, it learns how to place blocks stably, and how to stack blocks},
author = {Ngo, Hung and Luciw, Matthew and Forster, Alexander and Schmidhuber, Juergen},
doi = {10.1109/IJCNN.2012.6252824},
file = {:Users/ge/Documents/Mendeley/Ngo et al/Ngo et al., 2012, Proceedings of the International Joint Conference on Neural Networks, Learning skills from play Artificial curiosity o.pdf:pdf},
isbn = {9781467314909},
issn = {2161-4393},
journal = {Proceedings of the International Joint Conference on Neural Networks},
mendeley-groups = {curiosity},
pages = {10--15},
title = {{Learning skills from play: Artificial curiosity on a Katana robot arm}},
year = {2012}
}
@article{Graziano2011,
abstract = {Curiosity is an essential driving force for science as well as technology, and has led mankind to explore its surroundings, all the way to our current understanding of the uni-verse. Space science and exploration is at the pinnacle of each of these developments, in that it requires the most advanced technology, ex-plores our world and outer space, and con-stantly pushes the frontier of scientific knowl-edge. Manned space missions carry dispro-portionate costs and risks, so it is only natural for the field to strive for autonomous explo-ration. While recent innovations in engineer-ing, robotics and AI provide solutions to many sub-problems of autonomous exploration, in-sufficient emphasis has been on the higher level question of autonomously deciding what to explore. Artificial curiosity, the subject of this paper, precisely addresses this issue. We will introduce formal notions of " interesting-ness " based on the concepts of (1) compres-sion progress through discovery of novel regu-larities in the observations, and (2) coherence progress through selection of data that " fits " the already known data in a compression-based way. Further, we discuss how to con-struct a system that exhibits curiosity driven by the interestingness of certain types of novel observations, with the mission to curiously go where no probe has gone before.},
author = {Graziano, Vincent and Glasmachers, Tobias and Schaul, Tom and Pape, Leo and Cuccu, Giuseppe and Leitner, Jurgen and Schmidhuber, J{\"{u}}rgen},
doi = {10.2420/AF04.2011.41},
file = {:Users/ge/Documents/Mendeley/Graziano et al/Graziano et al., 2011, Acta Futura, Artificial Curiosity for Autonomous Space Exploration.pdf:pdf},
journal = {Acta Futura},
mendeley-groups = {curiosity},
number = {1},
pages = {41--51},
title = {{Artificial Curiosity for Autonomous Space Exploration}},
url = {http://dx.doi.org/10.2420/AF04.2011.41},
volume = {1},
year = {2011}
}
@article{Schmidhuber1991,
abstract = {Wie k{\"{o}}nnen Maschinen zielgerichtet und selbstgesteuert lernen? Durch Neugier. Je gr{\"{o}}{\ss}er die M{\"{o}}glichkeit, bereits bestehende Data-Compressors zu verbessern, desto gr{\"{o}}{\ss}er die Neugier.},
author = {Schmidhuber, J{\"{u}}rgen},
file = {:Users/ge/Documents/Mendeley/Schmidhuber/Schmidhuber, 1991, Meyer, J.A. and Wilson, S.W. (eds) From Animals to animats, A Possibility for Implementing Curiosity and Boredom in.pdf:pdf},
isbn = {0-262-63138-5},
journal = {Meyer, J.A. and Wilson, S.W. (eds) : From Animals to animats},
mendeley-groups = {curiosity},
pages = {222--227},
title = {{A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers}},
year = {1991}
}
@article{Schmidhuber2015,
abstract = {This paper addresses the general problem of reinforcement learning (RL) in partially observable environments. In 2013, our large RL recurrent neural networks (RNNs) learned from scratch to drive simulated cars from high-dimensional video input. However, real brains are more powerful in many ways. In particular, they learn a predictive model of their initially unknown environment, and somehow use it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic information theory, we describe RNN-based AIs (RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending sequences of tasks, some of them provided by the user, others invented by the RNNAI itself in a curious, playful fashion, to improve its RNN-based world model. Unlike our previous model-building RNN-based RL machines dating back to 1990, the RNNAI learns to actively query its model for abstract reasoning and planning and decision making, essentially "learning to think." The basic ideas of this report can be applied to many other cases where one RNN-like system exploits the algorithmic information content of another. They are taken from a grant proposal submitted in Fall 2014, and also explain concepts such as "mirror neurons." Experimental results will be described in separate papers.},
archivePrefix = {arXiv},
arxivId = {1511.09249},
author = {Schmidhuber, Juergen},
doi = {1511.09249v1},
eprint = {1511.09249},
file = {:Users/ge/Documents/Mendeley/Schmidhuber/Schmidhuber, 2015, Unknown, On Learning to Think Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Control.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
mendeley-groups = {curiosity},
pages = {1--36},
pmid = {1000303116},
title = {{On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models}},
url = {http://arxiv.org/abs/1511.09249},
year = {2015}
}
@article{Storck1995,
abstract = {For an agent living in a non-deterministic Markov environment (NME), what is, in theory, the fastest way of acquiring information about its statistical properties? The answer is: to design "optimal" sequences of "experiments" by performing action sequences that maximize expected information gain. This notion is implemented by combining concepts from information theory and reinforcement learning. Experiments show that the resulting method, reinforcement driven information acquisition, can explore certain NMEs much faster than conventional random exploration.},
author = {Storck, Jan and Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
file = {:Users/ge/Documents/Mendeley/Storck, Hochreiter, Schmidhuber/Storck, Hochreiter, Schmidhuber, 1995, Proceedings of the International {\ldots}, Reinforcement driven information acquisition in non-determi.pdf:pdf},
journal = {Proceedings of the International {\ldots}},
keywords = {Exploration,Q-learning,Reinforcement learning,information gain,maximum likelihood models,non-deterministic Markovian environments,reinforcement directed information acquisition},
mendeley-groups = {curiosity},
pages = {159--164},
title = {{Reinforcement driven information acquisition in non-deterministic environments}},
volume = {2},
year = {1995}
}
@article{Sun2011,
abstract = {To maximize its success, an AGI typically needs to explore its initially unknown world. Is there an optimal way of doing so? Here we derive an affirmative answer for a broad class of environments.},
archivePrefix = {arXiv},
arxivId = {1103.5708},
author = {Sun, Yi and Gomez, Faustino and Schmidhuber, J{\"{u}}rgen},
doi = {10.1007/978-3-642-22887-2_5},
eprint = {1103.5708},
file = {:Users/ge/Documents/Mendeley/Sun, Gomez, Schmidhuber/Sun, Gomez, Schmidhuber, 2011, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lectu.pdf:pdf},
isbn = {9783642228865},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {curiosity},
pages = {41--51},
title = {{Planning to be surprised: Optimal Bayesian exploration in dynamic environments}},
volume = {6830 LNAI},
year = {2011}
}
