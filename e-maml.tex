\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage[fleqn]{amsmath}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multicol}
\usepackage{float}
\usepackage{algorithm2e}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{enumitem}
%\setlength{\belowcaptionskip}{-10pt}



\title{Some Considerations on Learning to Explore via Meta-Reinforcement Learning}
%%PA: possible alternative titles: 
% Learning to Explore through Meta Reinforcement Learning
% Learning to Explore
% Rein: def go with 'Learning to Explore'
% BCS: Wait both of you want to just call the paper 'Learning to Explore.' with nothing else? I feel like it misses the whole meta learning aspect of the paper. 
%%PA: "learning to explore" focuses on the goal / achievements of the paper, so seems more appealing;  I could see "Learning to Explore through Meta Reinforcement Learning" be a good way to both have the appeal of what's being achieved + say something about the methodology  [current title "exploration in meta RL" is misleading -- as it's not about exploration of the meta-learner, but rather exploration of the inner learner;  certainly in the future there should be another paper with this title, it's just not this one]
%% BCS: Where we're at with this. I don't like the word benchmarking in the title. I also think that learning to explore is going to give the paper really high expectations that it doesn't meet. 
%% BCS: I still think I'm going to get skewered for this title when the reviewers expect a lot out of this paper. 

\author{$\text{Bradly C. Stadie}^{1, 2}$, $\text{Ge Yang}^{3}$, $\text{Rein Houthooft}^{1}$, $\text{Xi Chen}^{4}$, $\text{Yan Duan}^{4}$ \\$\textbf{Yuhuai Wu}^{5}$, $\textbf{Pieter Abbeel}^{4, 6}$, $\textbf{Ilya Sutskever}^{1}$  \\ 
$^1$ OpenAI\\
$^2$ UC Berkeley, Department of Statistics\\
$^3$ University of Chicago, Department of Physics \\
$^4$ UC Berkeley, Departments of EECS\\
$^5$ University of Toronto \\
$^6$ ICSI \\
\texttt{bstadie@openai.com} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\sset}{\mathcal{S}}
\newcommand{\aset}{\mathcal{A}}
\newcommand{\trans}{\mathcal{P}}


%\iclrfinalcopy % Uncomment for camera-ready version

\begin{document}





\maketitle

\begin{abstract}
% BCS: This is the perfect length and should not be increased in length at all. Do not add any length to this abstract. If you wish to add something, please remove something else. 
We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and E-$\text{RL}^2$. Results are presented on a novel environment we call `Krazy World'  and a set of maze environments. We show E-MAML and E-$\text{RL}^2$ deliver better performance on tasks where exploration is important.
%R: would remove listing of envs; what is performance? Describe the E-
%BCS: That stuff goes in the introduction. 
%%PA: some adjustment will be needed if/when title changes
%%BCS: A reminder that adding any length to this abstract is punishable by loss of tenure. 
\end{abstract}

\section{Introduction}
Supervised learning algorithms typically have their accuracy measured against some held-out test set which did not appear at training time. A supervised learning model generalizes well if it maintains its accuracy under data that has non-trivial dissimilarity from the training data. This approach to evaluating algorithms based on their generalization ability contrasts the approach in reinforcement learning (RL), wherein there is usually no distinction between training and testing environments. Instead, an agent is trained on one environment, and results are reported on this same environment. Most RL algorithms thus favor mastery over generalization. 

Meta RL is the suggestion that RL should take generalization seriously. In Meta RL, agents are evaluated on their ability to quickly master new environments at test time. Thus, a meta RL agent must not learn how to master the environments it is given, but rather it must \textit{learn how to learn} so that it can quickly train at test time. Recent advances in meta RL have introduced algorithms that are capable of generating large policy improvements at test time with minimal sample complexity requirements \cite{maml, rl2, learntorl}.   

One key question for meta RL that has hitherto been neglected in the literature is that of exploration. A crucial step in learning to solve many environments is an initial period of exploration and system identification. Furthermore, we know that real-life agents become better at this exploratory phase with practice. Consider, for example, an individual playing an entirely new video game. They will first need to identify the objective of the game and its mechanics. Further, we would expect that individuals who have played many video games would have a significantly easier time learning new games. Similarly, we would expect a good meta RL agent to become more efficient at this exploratory period. Unfortunately, we have found existing algorithms deficient in this area. We hypothesize that this failure can at least partially be attributed to the tendency of existing algorithms to do greedy local optimizations at each step of meta-training, as discussed further below. 

To begin addressing the problem of exploration in meta RL, we introduce two new algorithms: E-MAML and $\text{E-RL}^2$. It should come as no surprise that these algorithms are similar to their respective namesakes MAML and $\text{RL}^2$. The algorithms are derived by reformulating the underlying meta-learning objective to account for the impact of initial sampling on future (post-meta-updated) returns. We show that our algorithm achieves better results than MAML and $\text{RL}^2$ on two environments: a Krazy World environment we developed to benchmark meta RL, and a set of maze environments. Code for these benchmark environments, as well as code for new and faster implementations of E-MAML, MAML, $\text{E-RL}^2$, and $\text{RL}^2$ will be released upon this paper's publication. 

%%PA: last paragraph of intro could be along the lines of:  The main contributions of this paper are two end-to-end formulations for learning to explore.  One formulation builds on top of MAML [cite], the other formulation builds on top of RL2 / Learning-to-RL [cite cite].  Our experiments on "gridworlds", Super Nintendo games, and Super Mario show that by explicitly considering exploration in the meta-learning objective the resulting agents are able to (i) explore new environments more effectively, (ii) perform meta-reinforcement learning with better sample efficiency.  Our agents are able to master new game levels from  [something about number of trials + compare with if just PPO from scratch in a level?] 

\section{Related Work} 
Recently, a flurry of new work in Deep Reinforcement Learning has provided the foundations for tackling RL problems that were previously thought intractable. This work includes: 1) \cite{dqn, a3c}, which allow for discrete control in complex environments directly from raw images. 2) \cite{trpo, a3c, ppo, ddpg}, which have allowed for high-dimensional continuous control in complex environments from raw state information. \\
\\
Although these algorithms offer impressive capabilities, they still often falter when faced with environments where exploration presents a significant challenge. This is not surprising, as there exists a large body of RL work addressing the exploration problem \citep{kearns, rmax, beb}. In practice, these methods are often not used due to difficulties with high-dimensional observations, difficulty in implementation on arbitrary domains, and lack of promising results. This resulted in most deep RL work utilizing epsilon greedy exploration \citep{dqn}, or perhaps a simple scheme like Boltzmann exploration \citep{boltzman}. As a result of these shortcomings, a variety of new approaches to exploration in deep RL have recently been suggested \citep{hash, vime, stadie, bootstrapdqn, countbased, countbased2}. Further, there has been extensive research on the idea of artificial curiosity. For example \cite{Ngo2012, Graziano2011, Schmidhuber1991, Schmidhuber2015, Storck1995, Sun2011} all deal with teaching agents to generate a curiosity signal that aids in exploration. Although this line of work does not explicitly deal with exploration in meta learning, it remains a large source of inspiration for this work. In spite of the numerous cited efforts in the above paragraph, the problem of exploration in RL remains difficult. \\
\\
This paper will consider the problem of exploration in meta RL. However, it is important to note that many of the problems in meta RL can alternatively be addressed with the field of hierarchical reinforcement learning. In hierarchical RL, a major focus is on learning primitives that can be reused and strung together. These primitives will frequently enable better exploration, since they'll often relate to better coverage over state visitation frequencies. Recent work in this direction includes \citep{feudal, optioncritic, minecraft, progressive}. The primary reason we consider meta learning over hierarchical RL is that we find hierarchical RL tends to focus more on defining specific architectures that should lead to hierarchical behavior, whereas meta learning instead attempts to directly optimize for these behaviors. 

As for meta RL itself, the landscape is much less mature than deep RL and hierarchical RL. As a consequence, the available methods are more limited. To the best of our knowledge, there does not exist any literature addressing the topic of exploration in meta RL. This paper will seek to begin that discussion by considering E-MAML and  $\text{E-RL}^2$, which build upon the existing meta learning algorithms MAML \citep{maml} and $\text{RL}^2$ \citep{rl2}. \\
\\
In MAML, tasks are sampled and a policy gradient update is computed for each task with respect to a fixed initial set of parameters. Subsequently, a meta update is performed where a gradient step is taken that moves the initial parameter in a direction that would have maximally benefited the average return over all of the sub-updates. Results are presented on an environment wherein a simulated ant must determine its goal direction, which changes each time a new task is sampled. In $ \text{RL}^2$, an RNN is made to ingest multiple rollouts from many different MDPs and then perform a policy gradient update through the entire temporal span of the RNN. The hope is that the RNN will learn a faster RL algorithm in its memory weights. 
% BCS: Does this paragraph go here? It seems kind of awkward. 




\section{Reinforcement Learning Notation}
Let $M = (\sset, \aset, \trans, r, \rho_0, \gamma, T)$ represent a discrete-time finite-horizon discounted Markov decision process (MDP). The elements of $M$ have the following definitions: $\sset$ is a state set, $\aset$ an action set, $\trans: \sset \times \aset \times \sset \rightarrow \mathbb{R}_{+}$ a transition probability distribution, $r: \sset \times \aset \rightarrow \mathbb{R}$ a reward function, $\rho_0: \sset \to \mathbb{R}_+$ an initial state distribution, $\gamma \in [0, 1]$ a discount factor, and $T$ the horizon. We will sometimes speak of $M$ having a loss function $\mathcal{L}$ rather than reward function $r$. All we mean here is that $\mathcal{L}(s) = -r(s)$

In a classical reinforcement learning setting, we optimize to obtain a set of parameters $\theta$ which maximize the expected discounted return under the policy $\pi_{\theta}: \sset \times \aset \to \mathbb{R}_+$. That is, we optimize to obtain $\theta$ that maximizes $\eta(\pi_\theta) = \mathbb{E}_{\pi_\theta}[ \sum_{t=0}^T \gamma^t r(s_t) ]$, where $\displaystyle s_0 \sim \rho_0(s_0)$, $a_t \sim \pi_\theta(a_t|s_t)$, and $s_{t+1} \sim \trans(s_{t+1} | s_t, a_t)$.   %%PA: c -> r ?



\section{Problem Formulation and Algorithms}

\subsection{The Meta Reinforcement Learning Objective}
In meta reinforcement learning, we consider a family of MDPs $\mathcal{M} = \{M_i \}_{i=1}^N$ which comprise a distribution of tasks. The goal of meta RL is to find a policy $\pi_\theta$ and paired update method $U$ such that, given $M_i \sim \mathcal{M}$, we have $\pi_{U(\theta)}$ solves $M_i$ quickly. The word quickly is important here, so let us elaborate. By quickly, we mean orders of magnitude more sample efficient than simply solving $M_i$ with policy gradient or value iteration methods. In fact, an ideal meta RL algorithm would solve these tasks by collecting 1-10 trajectories from the environment for tasks where policy gradients require 100,000 or more trajectories. The thinking here goes that if an algorithm can solve a problem with so few samples, then it might be `learning to learn.' That is, the agent is not learning how to master a given task but rather how to quickly master new tasks. We can write this objective cleanly as 
\begin{align}
    \min_\theta \sum_{M_i} \mathbb{E}_{(\pi_{U(\theta)})} \left[ \mathcal{L}_{M_i} \right]
    \label{eq:objective}
\end{align}

Note that this objective is similar to the one that appears in MAML \cite{maml}, which we will discuss further below. 


\subsection{E-MAML} 

We can expand the expectation from (\ref{eq:objective}) into the integral 
\begin{align}
\int_{\tau \sim \pi_{U(\theta)}} R(\tau) \pi_{U(\theta)} (\tau) d \tau \label{eq:int}
\end{align} 
The objective (\ref{eq:objective}) can be optimized by taking a derivative of this integral with respect to $\theta$ and carrying out a standard REINFORCE style analysis to obtain a tractable expression for the gradient \cite{reinforce}. \\
\\
One issue with this direct optimization is that it will not account for the impact of the original sampling distribution $\pi_\theta$ on the future rewards $R(\tau), \tau \sim \pi_{U(\theta)}$. We would like to account for this, because it would allow us to weight the initial samples $\bar{\tau} \sim \pi_\theta$ by the expected future returns we expect after the sampling update $R(\tau)$. We argue that optimizing for this impact will make the resulting policy more exploratory. This is because when the dependence is not accounted for, $\pi_\theta$ will collect samples that lead to the largest local improvement after $U(\theta)$. However, when the dependence is accounted for, $\pi_\theta$ will be encouraged to also collect samples that lead to good meta-updates. %%PA: I know Maruan also called the inner updates "meta-updates" but I think he should probably fix that, and we probably shouldn't call them "meta-updates" either b/c the term "meta-update" seems better to be associated with the outer gradient rather than inner
Including this dependency can be done by writing the modified expectation as

\begin{align}
\int_{\tau \sim \pi_{U(\theta)}} \int_{\bar{\tau} \sim \pi_\theta} R(\tau) \pi_{U(\theta)} (\tau) \pi_\theta (\bar{\tau}) d \tau d \bar{\tau} \label{eq:int2}
\end{align}

Note that one could certainly argue that this is the correct form of (\ref{eq:int}), and that the failure to include the dependency above makes (\ref{eq:int}) incomplete. We choose to view both as valid approaches, with (\ref{eq:int2}) simply being the more exploratory version of the objective for reasons discussed above. 
%%PA: torn on the above paragraph; I agree with the truth contents of what's said there, just wondering how reviewers will react

%%PA: I think the explicit form below is more transparent than the math above, and I think the math above can be skipped, just right away present the math below; making the equation below the objective [ridding off the derivative up front];  I also wonder if maybe the objective is general across both E-MAML and E-RL2, and maybe could be its own (short) subsection, formulating what we are trying to optimize ;  and then from there specialize into two meta-learning approaches for optimizing this objective: E-MAML (which assumes a certain structure to how the pre / post policies relate to each other, namely through a predefined Update function (most typically a gradient-based update)); and E-RL2, which imposes less structure on the relationship between pre / post


In any case, we find ourselves wishing to find a tractable expression for the gradient of (\ref{eq:int2}). This can be done quite smoothly by applying the product rule under the integral sign and going through the REINFORCE style derivation twice to arrive at a two term expression, one of which encourages exploitation and one of which encourages exploration.

\begin{align*}
& \frac{\partial}{\partial \theta} \int_{\tau \sim \pi_{U(\theta)}} \int_{\bar{\tau} \sim \pi_\theta} R(\tau) \pi_{U(\theta)} (\tau) \pi_\theta (\bar{\tau}) d \tau d \bar{\tau} \\
&= \int_{\tau \sim \pi_{U(\theta)}} \int_{\bar{\tau} \sim \pi_\theta} R(\tau) \left[ \pi_\theta(\bar{\tau}) \frac{\partial}{\partial \theta} \pi_{U(\theta)} (\tau) + \pi_{U(\theta)}(\tau) \frac{\partial}{\partial \theta} \pi_\theta (\bar{\tau}) \right] d \tau d \bar{\tau} \\
& \approx \frac{1}{T} \sum_{i=1}^T R(\tau^{i}) \frac{\partial}{\partial \theta} \log \pi_{U(\theta)} (\tau^i) + \frac{1}{T} \sum_{i=1}^T R(\tau^{i}) \frac{\partial}{\partial \theta} \log \pi_{\theta} (\bar{\tau}^i) \hspace{3mm} \tau^i \sim \pi_{U(\theta)} \hspace{3mm} \bar{\tau}^i \sim \pi_\theta 
\end{align*}
Note that, if we only consider the term on the left, we arrive at the original MAML algorithm \cite{maml}. This term encourages the agent to take update steps $U$ that achieve good final rewards. It is exploitative. The second term encourages the agent to take actions such that the eventual meta-update yields good rewards (crucially, it does not try and exploit the reward signal under its own trajectory $\bar{\tau}$). This term allows the policy to be more exploratory, as it will attempt to deliver the maximal amount of information useful for the future rewards $R(\tau)$ without worrying about its own rewards $R(\bar{\tau})$. Since this algorithm augments MAML by adding in an exploratory term, we call it E-MAML. 


\subsection{E-$\text{RL}^2$}
$\text{RL}^2$ optimizes (\ref{eq:objective}) by feeding multiple rollouts from multiple different MDPs into an RNN. The hope is that the RNN hidden state update $h_t = C(x_t, h_{t-1})$, will learn to act as the update function $U$. Then, performing a policy gradient update on the RNN will correspond to optimizing the meta objective (\ref{eq:objective}). 

We can write the $\text{RL}^2$ update rule more explicitly in the following way. Suppose $L$ represents an RNN. Let $\text{Env}_k(a)$ be a function that takes an action, uses it to interact with the MDP representing task $k$, and returns the next observation $o$, reward $r$, and a termination flag $d$. Then we have 
\begin{align*}
    x_t &= \left[o_{t-1}, a_{t-1}, r_{t-1}, d_{t-1} \right] \\
    L(h_t, x_t) &= \left[ a_t, h_{t+1}\right] \\
    \text{Env}_k(a_t) &= \left[ o_t, r_t, d_t \right]
\end{align*}
To train this RNN, we sample $N$ MDPs from $\mathcal{M}$ and obtain $k$ rollouts for each MDP by running the MDP through the RNN as above. We then compute a policy gradient update to move the RNN parameters in a direction which maximizes the returns over the $k$ trials performed for each MDP. 

To make this algorithm more exploratory, we can take inspiration from the insights shone by E-MAML. For each MDP $M_i$, we will sample $p$ exploratory rollouts and $k-p$ non-exploratory rollouts. During an exploratory rollout, the forward pass through the RNN will receive all information. However, during the backwards pass, the future discounted returns for the policy gradient computation will zero out the contributions from exploratory episodes. The thinking is that this will encourage the RNN to treat its initial rollouts as exploratory, taking actions which may not lead to immediate rewards but rather to the RNN hidden weights performing better system identification, which will lead to higher rewards in later episodes.     


\section{Experiments}
\subsection{Krazy World Environment} 
To test the ability of meta RL algorithms to explore, we introduce a new environment known as Krazy World. This environment has the following features: 
\begin{enumerate}
    \item \textbf{8 different tile types}: Goal squares provide +1 reward when retrieved. The agent reaching the goal does not cause the episode to terminate, and there can be multiple goals. Ice squares will be skipped over in the direction the agent is transversing. Death squares will kill the agent and end the episode. Wall squares act as a wall, impeding the agent's movement. Lock squares can only be passed once the agent has collected the corresponding key from a key square. Teleporter squares transport the agent to a different teleporter square on the map. Energy squares provide the agent with additional energy. If the agent runs out of energy, it can no longer move. The agent proceeds normally across normal squares. 
    \item \textbf{Ability to randomly swap color palette}: The color palette for the grid can be randomly permuted, changing the color that corresponds to each of the different tile types. The agent will thus have to identify the new system to achieve a high score. Note that in representations of the gird wherein basis vectors are used rather than images to describe the state space, each basis vector will correspond to a tile type, and permuting the colors will correspond to permuting the types of tiles these basis vectors represent. We prefer to use the basis vector representation in our experiments, as it is more sample efficient.
    \item \textbf{Ability to randomly swap dynamics}: The game's dynamics can be altered. The most naive alteration simply permutes the player's inputs and corresponding actions (issuing the command for down moves the player up etc). More complex dynamics alterations allow the agent to move multiple steps at a time in arbitrary directions, making the movement more akin to that of chess pieces. 
    \item \textbf{Local or Global Observations}: The agent's observation space can be set to some fixed number of squares around the agent, the squares in front of the agent, or the entire grid. Observations can be given as images or as a grid of basis vectors. For the case of basis vectors, each element of the grid is embedded as a basis vector that corresponds to that tile type. These embeddings are then concatenated together to form the observation proper.  We will use local observations. 
    \end{enumerate}
    
    A successful meta learning agent will first need to explore the environment, identifying the different tile types, color palette, and dynamics. A meta learning algorithm that cannot learn to incorporate exploration into its quick updating strategy will ultimately have insufficient data to navigate the game. We see this empirically in our experiments below. 
    
\begin{figure}[H]
\begin{center}
\includegraphics[width=30mm]{envs/grid_0.png}\hfill
\includegraphics[width=30mm]{envs/grid_1.png}\hfill
\includegraphics[width=30mm ]{envs/grid_2.png} 
\end{center}
\caption{Three example worlds drawn from the task distribution. The agent must first perform an exploratory stage of system identification before exploiting. For example, in the leftmost grid the agent must first identify that the orange squares give +1 reward and the blue squares replenish its limited supply of energy. Further, it will need to identify that the gold squares block progress and the black square can only be passed by picking up the pink key. The agent may also want to identify that the brown squares will kill it and that it will slide over the purple squares. The other center and right worlds show these assignments will change and need to be re-identified every time a new task is drawn.} %%PA: love this figure!  Would be helpful to explicitly explain one of the three example Krazy Gridworlds, so someone seeing just the figure + caption can fully understand the game
\end{figure} 

\begin{figure}[H]
\begin{center}
\includegraphics[height=20mm]{envs/grid_0.png} %
\includegraphics[height=20mm]{envs/grid_local.png}  
\end{center}
\caption{A comparison of local and global observations for the Krazy World environment. In the local mode, the agent only views a $3 \times 3$ grid centered about itself. In global mode, the agent views the entire environment.}
\end{figure} 

    
\subsection{Mazes}
A collection of maze environments. The agent is placed at a random square within the maze and must learn to navigate the twists and turns to reach the goal square. A good exploratory agent will spend some time learning the maze's layout in a way that minimizes repetition of future mistakes. The mazes are not rendered, and consequently this task is done with state space only. The mazes are $20 \times 20$ squares large.   
\begin{figure}[H]
\begin{center}
\includegraphics[width=40mm, height=25mm]{envs/maze.png} 
\end{center}
\caption{One example maze environment rendered in human readable format. The agent attempts to find a goal within the maze.}
\end{figure} 

%\subsection{Super Mario}
%\begin{figure}[H]
%\begin{center}$
%\begin{array}{ccc}
%\includegraphics[width=50mm]{envs/mario_0.png} &
%\includegraphics[width=50mm]{envs/mario_1.png} & %\includegraphics[width=50mm ]{envs/mario_2.png} 
%\end{array}$
%\end{center}
%\caption{Three example Mario environments. The agent will need to adapt to different color schemes, different enemies, and different layouts for different levels}
%\end{figure} 

    
\subsection{Results} 
In this section we present the following experimental results. 
\begin{enumerate} 
\item Learning curves on Krazy World and mazes. 
\item The gap between the agent's initial performance on new environments and its performance after updating. A good meta learning agent will have a large gap after updating. A standard RL agent will have virtually no gap after one update. 
\item Two heuristics which are designed to measure how well all of the algorithms are exploring. 
\item Additionally, the following results can be found in the appendix: 
\begin{enumerate}
    \item Curves that show variance on returns of all four meta learning algorithms  over 16 different experimental seeds.  
    \item More learning, variance, and first-update-performance-gap curves for Krazy World. These curves use different seeds to sample the testing environments. To convince ourselves that our results work without over-fitting to the test distribution, no hyper-parameter tuning was done on these seeds.  
    \item An analysis of the number of gradient steps taken vs. the final returns for MAML and E-MAML. Since we see that returns are not improved dramatically with additional gradient steps, we present results on one gradient step in this section. 
\end{enumerate}
\end{enumerate}


Figure \ref{fig:learning-curves-0} presents learning curves on Krazy World for MAML, E-MAML, $\text{RL}^2$, and E-$\text{RL}^2$. Results are averaged over 16 randomly sampled hyper parameter choices for all algorithms (See the appendix for a detailed listing of hyper-parameters. As noted in this section, the algorithmic seed is considered a hyper-parameter in these experiments). The Y axis is the reward obtained after training at test time on a set of 64 held-out test environments. For MAML and E-MAML, this means performing one gradient step at test time. For $\text{RL}^2$ and E-$\text{RL}^2$, this means running three exploratory rollouts to allow the RNN memory weights time to update and then reporting the loss on the fourth and fifth rollouts. The X axis is the total number of environmental time-steps the algorithm has used for training. Every time an action is handed to an environment during training and that environment advances forward, this count increments by one. 

\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.4]{bradly_curves/64testgrid0.png} 
\end{center}
\caption{Meta learning curve on Krazy World. We see that E-$\text{RL}^2$ is at times closest to the optimal solution (obtained via value iteration), but has higher variance. E-MAML converges faster than MAML, although both algorithms do manage to converge. Meanwhile, $\text{RL}^2$ cannot converge in the given time. However, it is possible this curve would converge if given more time.}
\label{fig:learning-curves-0}
\end{figure} 

Learning curves for mazes are presented in Figure \ref{fig:learning-curves-1}. Here, the story is different than Krazy World. $\text{RL}^2$ and E-$\text{RL}^2$ both perform better than MAML and E-MAML. We suspect the reason for this is that RNNs are able to leverage memory, which is more important in mazes than in Krazy World. It is interesting to note that  E-$\text{RL}^2$ was still more successful than  $\text{RL}^2$, though both algorithms do converge to similar policies. It is interesting to note that this environment carries a penalty for hitting the wall, which MAML and E-MAML discover immediately. However, it takes E-$\text{RL}^2$ and $\text{RL}^2$ much longer to discover this penalty, resulting in worse performance at the beginning of training. 
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.4]{bradly_curves/64_maze_curves.png}
\end{center}
\caption{Meta learning curve on mazes. The environment gives the agent a penalty for hitting the wall. Interestingly, E-$\text{RL}^2$ and $\text{RL}^2$ take much longer to learn how to avoid the wall. However, they also deliver better final performance. E-$\text{RL}^2$ learns faster than $\text{RL}^2$, but offers comperable final performance. Since MAML and E-MAML utilize MLPs and have no memory, we do not expect good performance on this task.}
\label{fig:learning-curves-1}
\end{figure} 

 
When examining meta learning algorithms, one important metric is the update size after one learning episode. A good meta learning algorithm should have a large gap between its initial policy, which is largely exploratory, and its updated policy, which should often solve the problem entirely. For MAML, we look at the gap between the initial policy and the policy after one policy gradient step (see the appendix for information on further gradient steps). For $\text{RL}^2$, we look at the results after three exploratory rollouts, which give the RNN hidden state $h$ sufficient time to update. Note that this number of exploratory rollouts was used during training as well. As baselines, we also plot the average gap in performance for two rollouts of a random policy, and the average gap in a PPO policy that is trained on only the test-time rollouts. 
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.335]{bradly_curves/gap_grids_maml_0.png}%
\includegraphics[scale=0.335]{bradly_curves/gap_grids_rl2_0.png} \\
\includegraphics[scale=0.335]{bradly_curves/e-maml-gap-mazes.png}%
\includegraphics[scale=0.335]{bradly_curves/mazes_gap.png}  
\end{center}
\caption{Gap in initial policy and post update policy for MAML/EMAML on Krazy World (top left), $\text{RL}^2$/E-$\text{RL}^2$ on Krazy World(top right), MAML/EMAML on mazes (bottom left) and $\text{RL}^2$/E-$\text{RL}^2$ on mazes (bottom right). We see that generally the MAML family is more stable than the $\text{RL}^2$ family for this task. For Krazy World, the gap between the initial policy and the updated policy is large, which indicates that at the very least the meta learning is able to do system identification correctly. For mazes, the gap is often less pronounced.}
\end{figure} 

Finally, in Figure \ref{fig:hs} we consider two heuristics for measuring the exploratory ability of the meta-learners. First, we consider the fraction of tile types visited by the agent at test time. The intuition is that good exploratory agents will visit and identify all different tile types so it can optimally solve the game. Second, we consider the number of times an agent visits a death tile at test time. Agents that are efficient at exploring should visit this tile type exactly once and then avoid it. We often find that more naive agents will run into these tiles repeatedly, causing their death many times over and a sense of pity in any onlookers. Overall, we see that the exploratory algorithms achieve better performance on these heuristics. However, we hope that in the future with further refinement to exploration in meta-learning, the results can be made more clear and convincing. 

\section{Conclusion} 
We considered the problem of exploration in meta reinforcement learning. Two new algorithms were derived and their properties were analyzed. In particular, we showed that these algorithms tend to learn more quickly and explore more efficiently than existing algorithms. It is likely that future work in this area will focus on meta-learning a curiosity signal which is robust and transfers across tasks. Perhaps this will enable meta agents which learn to explore rather than being forced to explore by mathematical trickery in their objectives. 


\begin{figure}[H]
\begin{center}
\includegraphics[height=40mm]{bradly_curves/tile_types.png}\hfill
\includegraphics[height=40mm]{bradly_curves/times_visited_death.png}
\end{center}
\caption{Two heuristics to measure exploration: Fraction of tile types visited during test time (left) and number of times killed at a death square during test time (right). We see that E-MAML is consistently the most diligent algorithm at checking every tile type during test time. Beyond that, things are fuzzy with $\text{RL}^2$ and MAML both checking most tile types at test time and E-$\text{RL}^2$ being exceptionally sporadic in this regard. Note that for this experiment, agents were given 8 rollouts at test time. Hence, the maximal number of expected deaths is 8. We see that most agents start around this number, and decrease to around 4 by the time they have converged.}
\label{fig:hs}
\end{figure} 

%\section*{Acknowledgements}
%BCS: Pieter can you acknowledge our funding here.
%Should this paper be accepted, this is where we would thank our funding agencies and a couple people for their insightful discussions. 

\newpage

\bibliography{iclr2017_conference}
\bibliographystyle{iclr2018_conference}

\newpage


\section{Appendix A: Additional curves}

\subsection{Variance curves for MAML, E-MAML, RL2, E-RL2}

Figure \ref{fig:vargrids0} and Figure \ref{fig:varmazes0} show the variance of learning curves over the 16 sampled hyper-parameters on the 64 selected test environments. These graphs are designed to show both the sensitivity of the algorithms to choices of hyper-parameters, as well as the variance of the algorithms themselves. \\
\\
We see that E-$\text{RL}^2$ and $\text{RL}^2$ start with the lowest variance. Unfortunately, their variance never decreases throughout training, suggesting the algorithms have not yet fully converged in this time-step regime. When we manually inspected the behavior of instances in E-$\text{RL}^2$ that yielded particularly high variance, we found that most of the bad behaviors devolved to taking seemingly random actions. To us, this suggests that the $\text{RL}^2$ family of algorithms may be over-fitting to the training set. \\
\\
For MAML and E-MAML, the variance for E-MAML starts higher initially but then decreases and eventually converges much faster than MAML. Since E-MAML provides an unbiased estimate of the gradient and MAML does not, this behavior is in line with expectations. 
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.4]{bradly_curves/64testgridvar0.png} 
\end{center}
\caption{Variance of meta learning curves on Krazy World. E-MAML offers the best performance here, converging in around half the time it takes MAML. Meanwhile, the variance of E-$\text{RL}^2$ is the highest towards the end, which suggests over-fitting.}
\label{fig:vargrids0}
\end{figure} 

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.4]{bradly_curves/var_mazes.png} 
\end{center}
\caption{Variance of meta learning curves on mazes.}
\label{fig:varmazes0}
\end{figure}

\subsection{Learning, Variance, and First-update-performance-gap Curves for Three additional Test Environment Seeds}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.16]{bradly_curves/64testgrid0_scaled.png}%
\includegraphics[scale=0.16]{bradly_curves/64testgrid1.png}
\includegraphics[scale=0.16]{bradly_curves/64testgrid2.png}%
\includegraphics[scale=0.16]{bradly_curves/64testgrid3.png}
\end{center}
\caption{Learning curves for 4 different seeds of test environments. To convince ourselves that our results work without over-fitting to the test distribution, no hyper-parameter tuning was done on these seeds.}
\label{fig:appendix-learning-curves-0}
\end{figure} 

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.16]{bradly_curves/64testgridvar0_scaled.png}%
\includegraphics[scale=0.16]{bradly_curves/64testgridvar1.png}
\includegraphics[scale=0.16]{bradly_curves/64testgridvar2.png}%
\includegraphics[scale=0.16]{bradly_curves/64testgridvar3.png}
\end{center}
\caption{Variance curves for 4 different seeds of test environments.}
\label{fig:apprendix-variance-curves-1}
\end{figure} 

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.16]{bradly_curves/gap_grids_maml_0.png}%
\includegraphics[scale=0.16]{bradly_curves/gap_grids_maml_1.png}
\includegraphics[scale=0.16]{bradly_curves/gap_grids_maml_3.png}%
\includegraphics[scale=0.16]{bradly_curves/gap_grids_maml_4.png}
\end{center}
\caption{First-update-performance-gap curves for 4 different seeds of test environments. Results for MAML and E-MAML}
\label{fig:appendix-gap-curves-0}
\end{figure} 

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.16]{bradly_curves/gap_grids_rl2_0.png}%
\includegraphics[scale=0.16]{bradly_curves/gap_grids_rl2_1.png}
\includegraphics[scale=0.16]{bradly_curves/gap_grids_rl2_2.png}%
\includegraphics[scale=0.16]{bradly_curves/gap_grids_rl2_3.png}
\end{center}
\caption{First-update-performance-gap curves for 4 different seeds of test environments. Results for E-$\text{RL}^2$ and $\text{RL}^2$.}
\label{fig:appendix-gap-curves-1}
\end{figure} 


%\subsection{Meta Learning Curves Vs PPO Learning Curves for E-$\text{RL}^2$ and E-MAML}
%In this graph, we show a curve of PPO training on a single fixed environment along with the E-$\text{RL}^2$ and E-MAML performance on this singular environments. Note that the meta-learning algorithms are still meta-learning across a distribution of environments, and the X axis represents the total number of environmental time-steps they've ingested to accomplish their meta-training. This graph helps us compare the speed of meta-learning to the speed of normal RL. 
%\begin{figure}[h]
%\centering
%\includegraphics[scale=0.4]{bradly_curves/64testgrid0.png} 
%\caption{}
%\label{fig:64testgrid0}
%\end{figure} 

\subsection{MAML Rewards vs number of gradient steps} 
\begin{figure}[H]
\label{curves}
\begin{center}
\includegraphics[scale=0.16]{bradly_curves/emaml-5-0.png}%
\includegraphics[scale=0.16]{bradly_curves/maml-5-0.png}
\includegraphics[scale=0.16]{bradly_curves/emaml-10-0.png}%
\includegraphics[scale=0.16]{bradly_curves/maml-10-0.png}
\includegraphics[scale=0.16]{bradly_curves/emaml-20-0.png}%
\includegraphics[scale=0.16]{bradly_curves/maml-20-0.png}
\end{center}
\caption{MAML on the right and E-MAML on the left. A look at the number of gradient steps at test time vs reward on the Krazy World environment. We see that beyond one gradient step, the returns are usually non-existent for both MAML and E-MAML. Hence, we only show results for one gradient step in the experimental results section.}
\label{fig:maml-grad-steps}
\end{figure} 

%\section{Appendix B: Hyper-Parameters} 


%\subsection{Hyper-parameters} 
%All hyper-parameters will be made available when the code is released for these experiments. For the graphs reported above, a valid range was defined over each hyper-parameter and then we uniformly sampled values from these ranges.


\end{document}
